Topics in common with previous years
-Corpora, annotation, evaluation
-Ambiguity at all levels
-N-gram models, entropy, smoothing
-Noisy channel framework
-Spelling correction, edit distance
-HMMs, part-of-speech tagging
-Syntax, parsing algorithms, PCFGs, other grammar formalisms
-Lexical semantics:  word senses, semantic roles, distributional semantics

New since last year
-Updated discussion of evaluation, including crowdsourcing
-High-level overview of more modern smoothing methods
-More complete example of spelling correction (end-to-end system)
-Generalized discussion of EM (showing application in both spelling correction and HMMs)
-Text classification (tasks and methods)
-Dependency grammar and related algorithms
-More detail on semantic roles and distributional semantics

Eliminated since last year
-corpus markup
-mathematical details of backoff in N-gram models
-details of forward-backward algorithm for HMMs
-feature structure grammars
-pronoun resolution
-discourse coherence
-Machine Translation (PROBABLY)

Multilevel Ambiguity
  -word sense
  -part of speech
  -syntactic structure I saw a man with a telescope
    -the relationship between the words and clauses of a sentence
    -types:
      -local, contains ambigious phrase but has only one interpretation, is often resolved
      -global, has at least two interpretations no feature helps to distinquish
  -Quantifier scope (what is the scope of the quantifier like some all in the sentence)
  -Multiple: I saw her duck jej kaczke, ja robiaca jak kaczka
  
ZIPFS LAW
  the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word
  Regardless  of  how  large  our  corpus  is,  there  will  be  a  lot  of  infrequent  (and zero-frequency!) word
  
N-gram model (and LMs?)
  probability of sentence, probability of sequence of characters
  used for:
    -spelling correction 
    -automatic speech recognition
    -machine translation
    -predictive text completion
  how to use:
    -for sentence, probability of words of the sentence, word given rest of the words
    -use n-grams the probability of a word only depends on a fixed number of previous words (history)
    
    -P(mast | before, the) = C(before, the, mast)/C(before, the) C- Counts (just an integer)
    
    -to capture beginning/end of sequence augument the input <s> <s> FIRSTWORD when trigram
    -or treat all the things as a long squence P(He | LASTWORDOFPREVIOUSSENTENCE, .)
    
    -use negative log probabilities (called costs), add stuff instead of mult
  
MLE
  problems:
    -thinks that everything that hasn't occured will never occur
    -so if data is sparse (most of the time) don't use it
  
EVALUATION TYPES
  -Extrinsic:
    -evaluation in use, think of utility in a normal task that human user performs
  -Intrinsic:
    characterise performance according to the gold standard
    
Entropy and Cross-entropy
  -how many yes/no questions do we need to ask to find out the outcome
  -low entropy means low uncertainty
  -cross-entropy >= entropy, uncertainty of a model can be no less than the true uncertainty
  
  per-word cross-entropy is well approximated by:
    H(w1, w2, ... wn) = -1/n log(base2)Prob(w1, w2, ... wn)
    -1/7(lg2(P(I)) + lg2(P(spent|I) + ....) = -1/7(-0.1 -6.3 ...) = 6
    
  -perplexity:
    -LM performance is reported as perplexity which is 2^cross-entropy
    -The   average   branching   factor   at   each   decision   point,   if   our distribution were uniform ?!?!?!??!?
  
  -low cross-entropy can mean that model is good or the corpus is 'easy', only compare diff models on same corpus
  
Smoothing:
  -MLE makes the training data maximally  probable by making unseen data minimally probable
  -Add-One Laplace:
    -in large vocabulary size it steals to much from seen events
  -Add-alpha (Lidstone):
    -add aplha<1 instead of 1
    -how to pick alpha:
      -three-way split:
        -training data 80-90
        -held-out (or development) 5-10
        -test 5-10
      -train with different aplhas on training set
      -chose alpha that minimizes cross-entropy on dev (held-out) set
      -report final on test set
  -Good-Turing:
    -changes nominator
    -P(GT) = c*/N   c* = (c+1)(N(c+1))/N(c) N(c) ow many things occured with frequency c. N is total number of words
      Sam I am I am Sam I do not eat
        I 3          N(1) = 3 total 1*3 + 2*2 + 1 *3 = 10
        sam 2        N(2) = 2
        am 2         N(3) = 1 
        
        so for zero we estimate only probability that something unseen happens, it is N(1)/N so here 3/10
        do 1         so for one frequency adjusted count is c* = 2(2)/3 and the probability is c*/N so here 4/3/10
        not 1        
        eat 1        sp fpr two
    -problems with Good-Turing:
      -assumes that we know the vocabulary size
      -doesn't allow "holes" in the counts, N(100) = 0, how to estimate N(99)???  USE LINEAR REGERSSION
      -applies discounts (lower probability) even to high-frequency items
  -Interpolation:
    -combine higher and lower order N-grams:
      -high-order has better context but sparse
      -low-order limited context but good counts
    -P(INTERPOLATION)(three|I, spent) = lamba1*P(three) + lambda2*P(three|spent) + lambda3*P(three|I, spent)
    -lambda1 + lambda2 + ... + lambdan = 1
    -lambdas are called interpolation parameters or mixture weights
    -chose lamda that maximises perplexity (2^cross-entropy) on held-out (dev)
  -Back-Off:
    -if highest N-gram is present use it, if not pick lower order N-gram
    -each N-gram probability has to be weighted with backoff weight
    -weights have to add up to 1, complicated maths
    problems:
      -bigram New York high probability York is almost always following New so as an N-gram it should have low probability, it is not the case with backoff (and interpolation)
  -Kneser-Ney:
    -takes diversity of histories into account
    -N(1+)(dot_wi) = |{w(1-i) : c(w(1-i), wi) > 0}|
    -take maximum likelihood and replace raw counts with count of histories
  
Word similarity: 
  -two words C(salmon) >> C(swordfish) can P(salmon|caught two) can us anything about P(swordfish|caught two) ?
  -early idea:
    define class of words
  -now:
    -distributed language models
    -each word represented as high-dimensional vector (50-500)
    -similar words represtend by similar vectors
    -learn words representation (embeddings) such that words that behave similarly are close together in space
    -learning embeddings is extremly time-consuming
    -embeddings capture both semantic and syntactic similarity
    
Noisy channel model:
  -assume noise:
    -typed words as opposed to intended words
    -spoken words as opposed to acoustic signal(that is perfect)
    -L1 to L2 (machine translation)
  -argmax(y)P(y|x) = argmax(y)P(x|y)P(y)    P(y) distribution of words user intended to type  P(x) distribution of words user typed
  
Spelling correction system:
  -assume large dict of real words
  -generate list of words y that differ by z character from x
  -Compute P(x|y)P(y) for each y and return max y
  -assume that typed character xi depends only on intended character yi (ignore context) then:
    P(actualword|intendedword) = P(no|not) = P(n|n)P(o|o)P(-|t)
    
  -we want to get rid of hand alignments:
    -we can produce them using MED
    -but to estimate cost we need noise model
    -but until we have alignments we can't esimate the noise model
    -need to use EM (expectation maximization)
    
Expectation-Maximization:
  -initialize learning parameters to arbitrary values (set all costs = 1)
  -compute optimal values for variables (run MED to get alignments)
  -recompute parameters using computed values
  -repeat until convergence (to local optimum of likeligood function)

Likelihood function:
  -call parameters of the model alpha
  -we can calculate probability of the dataset P(data|alpha) (this is the likelihood)
  -may have many local optima
  
Minimum edit distance:
  -amount of deletion + substitution + insertion, MED(stall, table) = 3
    S T A L L -
    d | | s | i
    - T A B L E
  -use dynamic programming to find optimal solution (there are multiple) cost is O(len(str1)* len(str2)) for mem and space
  -used for spelling correction
  
Text classification:
  -spam detection
  -sentiment analysis (binary of multiway):
    -pos/neg, 1-5 stars
    -pro/con, pro/con/neutral
  -topic classification
  -authorship attribution
  -can use N-gram but it is not capable of using other kinds of features like POS tags
  -use bag of words (it doesn't take order of words into account)
  -prepare features:
    -binarize words
    -ignore stopwords
    -chose a small task-relevant set:
      -using a sentiment lexicon so words that are indication stuff like bad awful, adobrable
      -domain specific (memory, operating system for PC computer product reviews)
  -using naive bayes for text classification:
    pros:
      -easy to implement
      -fast to train and classify
      -doesnt require much training data
      -usually works well
      -pick as baseline
    cons:
      -feature are usually not independent given the class, given TRAVEL sun and beach more likely than sun and sking
      -overconfident (asking 5 friends some opinion when some got their from each other)
  -Maximum Entropy model (or MaxEnt or multinomial logistic regression or  log-linear model or single neuron classifier):
    -model P(c|d) (class given document) directly
    -feature are functions that depend on both observations and class:
      for example with 3 classes:
        f1: contains('ski') & c = 1           w1 = 1.2
        f2: contains('ski') & c = 2           w2= -0.2
        f3: contains('ski') & c = 3           ...
        f4: link_to('wikipedia.com') & c = 1
        f5: contains('wikipedia.com') & c = 2
        f6: contains('wikipedia.com') & c = 3
    -P(c|observeddata) = 1/Z exp(observeddata dotproduct allfunctionsForThatClass) Z is normalising factor
    -features are defined using templates
      contains(w) & c
      header_contains(w) & c
      header_contains(w) & link_in_header & c
      ...
      -instantiate with all possible words w and classes c
      -filter out features occuring very few times
    -train the model:
      -given annotated data
      -chose weights that make the make lables most probable under the model
      -called Conditional Maximum Likelihood Estimation CMLE
      -will overfit, so use regularization
    -Cons of MaxEnt:
      -multple iterations to gradually improve weight (gradient ascent)
      -time consuming

Part of speech:
  -Open class words nouns verbs... new ones are being added website email
  -Closed class words pronouns determiners ... limited number of those

POS taggining:
  -can be useful for:
    -text clasification
    -word sense dismabiguation
  -hard because:
    -ambigutiy glass of water/NOUN vs water/VERB the plants
    -sparse data:
      -words havent seen before
      -word-tag pairs havent seen before
  -how to tag:
    -take word into account (arrow is always a NOUN)
    -take into accout surrounding words (two determiners rarerly follow each other)
  -can be viewed as a probabilistic FSM with transitions between states (tags) and emit on each state (words)
    
Hidden Markoc Models HMMs:
  -can be used for:
    -POS taggining
    -Names entity recognition (label words as belonging to persons, orgzanizations...)
    -Inforamtion field segmentation (is this part refering to place of event? to price of event?)
  -assumes that each state only depends on the previous state (and on the word!??!)
  -P(TAGSequence|sentence) = P(T|S) = P(S|T)P(T)/P(S) = argmax(T) P(T|S) = argmax(T) P(S|T)P(T)
  -P(T) is the state transition sequence
  -P(S|T) are the emission probabilities
  -Viterbi algorithm
  -Unsupervised learning of transitions:
    -we dont know state sequence we cant find transitions
    -if we know transitions we can find the best state sequence
    -use EM initialize params, compute expected counts, set new params based on MLE on the expected counts
    -real counts - count each transition in true tag sequence
    -expected counts - if sequence Q has prob p count p for each transition in Q, add up fractional counts across all possible sequences
    -forward-backward can be used istead of EM

Baum-Welch forward-backward:
  -computes expected counts using foward-probabilities and backward probabilities
  -EM is more general
  
Viterbi:
  -can be though as decoding someone sends us sequence of tags but they are converted to words and we need to go back
  -exhaustive
  -O(T^2 N) time and O(NT) space      N length T possible tags
  -use negative log probs

Dynamic programming:
  -use when:
    -enumeration is not computationally possible
  -uses memoization to avoid recomputations
    
Annotation:
  -hard because:
    -grey areas between categories
    -ambiguous
  -provide guidelines
  -qualiti subject to training and experience
  -make many ppl annotate and mesaure inter-annotator agreement
  -raw agreement rate proportion of labels in agreement

Evaluation:
  -dev set (held-out) is for tuning parameters
  -use dev set for tuning in order to prevent overfitting
  -gold standard the truth the best human judgement about what correct is in a task
  -upper bound is the gold standard
  -lower bound is a simple model baseline
  -is the improvement within Statistical significance
  
Syntax:
  -long-range dependencies Sam ... sleeps, Dogs ... sleep
  -substitutability at the phrasal level: (phrasal substitutability)
    -substitude adjectives red -> blue
    -substitude noun phrases dogs->green mushrooms
  -well-formed is diff from meaningful
  
Context-free grammar: CFG
  -consists of:
    -terminals
    -non-terminals
    -Start symbol (is part of non-terminals)
    -rules non-terminal to terminal
  -structural ambiguity sentences that have more than one parse (they have different POSs)
  -attachment ambiguity (words have same POSs attached but tree can be produced differently)
  -global ambiguity - multiple analyses of a full sentence
  -local ambiguity - multiple analyses for a parts of sentence

Parsing:
  -directionality:
    -top-down
    -bottom-up
    -mixed
  -search strategy:
    -DFS:
      -costly if facing local ambiguity
      -use probabilistic model to pick best choices
    -BFS
    -best first search
  -dynamic programming:
    -with CFG any substring is independent of the rest of the parse, so we can avoid recomputing
    -multiple parses are explored at once
    -chart (well-formed substring table WFST)

Recursive Descent Parsing:
  -top down depth first
  -expands nonterminals until reaching a terminal
  -backtracks if no options available
  -if left-recursive can stuck

Shift-Reduce Parsin:
  -depth first BUT bottom-up
  -puts words from sentence onto a stack
  -when RHS of any rule matches reduces one or more elements from the top of the stack to a LHS of that rule 
  -maintains backtrack points
  
CKY:
  -dynamic programming
  -bottom up, breadth first
  -finds all possible parses
  -avoids recomputing
  
Probabilistic parsing: PCFG prob context free grammar
  -treebank grammars:
    -annotate real sentences with parse trees
    -get gramma and probabilities for each rule
    -generative
  -inside algorightm (computes the probability of the sentence)
  -inside-outside algorithm:
    -form of EM
    -learns grammar rules probs from unannotated sentences
  -use best first (use probabilities to chose)
  -how to score parts of the sentence (constituents):
    -if we just take raw scores then smaller constituents will almost always have higher scores
    -divide by the number of words in the constituent
    -still not guaranteed to find best parse
    
Lexical dependency:
  -She called on the student vs she called on the phone
  -PCFGs are context free so POT tag will have same prob regardless of where is appears
  -how to fix context free:
    -augument POS tag with parent information so NP^S (if in subject position, so parent is subject), NP^VP, NP^PP
    -augument POS tag with lexicalization, add lexical head of the phrase (of a part of the sentence, so first word of the current part of the sentence)
  -pros:
    -more specific
  -cons:
    -sparse
    -need to use complex smoothing
  evaluation:
    -count number of correctly and uncorrectly predicted constituents
  
Lexicalized Constituency Parse (Dependency parsing):
  -why is it important:
    -replacing word with another with same POS will not result in a different parsing decision
  -create new categories by adding the lexical head of the phrase (S-saw kids)
  -dependencies why/what 
  -content head
  -functional head
  -captures head->modifier relations (label edges with the name of the relation)
  -useful for information extraction tasks:
    -involves real-world relationships, present chains of dependency
  -sentence's dependency parse is projective if dependency parse can be drawn on top of the sentence without any crossing edge
  -CKY with Eisner alhorithm is O(G * n^3) (probably this belongs to conversion-parsing)
  -Transition-based Parsing O(n) time (only finds projective trees)
  -Graph-based Parsing O(n^2)
  
Semantics/Meaning/Sense:
  -lexical semantics: the meaning of individual words
  -sentential semantics: how word meanings combine
  -different senses chemical plant/plant
  -synonyms holiday vacation (synonymous words belong to the same synset)
  -hyponym (words that are refering to subset)
  -hypernym (words that are refering to superset)
  -ontology - A is-a B relations
  -gradation good better
  -similarity good cool
  -homonyms can be spelled the same but diff meaning bank of the river bank$$$ (not always clear diff with next)
  -multiple senses polysems (polysemous words belong to multiple synsets) (not always clear diff with prev)
  -need to inference, combine two informations, Poland is a country in central europe and central european countries have done something
  -

WordNet:
  -ontology db
  -synsets, sets of synonymous words, they are organized into a network by several kinds of relations:
    -Hypernymy (Is-A): hyponym ambulance is a kind of hypernym car
    -Meronymy (Part-Whole):  meronym air bag is a part of holonym car
  -antonym OPPOSITES
  -problems:
    -neologisms: hoodie facepalm
    -multiword expression pay attention
    -names microsoft
    
WSD word sense disambiguation:
  -givven polysemous word find the sense in a given context
  -what features to use:
    -direct neighbouring words
    -any content words in a 50 words window
    -syntactic role in a sense
    -topic of the text
    -POS, surrounding POSs
    -care about correlated features!

Semantic Classes:
  -named entity recogniction NER
  -supersense tagging (categorising PERSON LOCATION...)
  -APPLE as ORG vs FOOD
  -list of known names - gazetteers

Distributional semantics:
  -how big context is?
  -boolean counts?
  -how to mesaure similarity between vectors?
  -mutual information:
    -how much more/less likely the cooccurrence is than if the words were independent
    -log2(actual prob of seeing words x and y together / Predicted prob of seeing x and y together is they are independent)
    -positive PMI just 0 for pmi<=0 because for infrequent words not enough data to accurately determine negative PMIs

Semantic roles:
  -PropBank
    -simpler
    -more data
  -FrameNet
    -richer
    -less data
    -stanford dependencies
    -lexical units

Definitions:
  -gold labels (when performing Sentiment Analysis) are +/- attached to the sentence
  -estimation vs probability, estimation - we have not true probabilities we throw coin 10 times and mesaure
  -alignmens - misspelled word to real word effert - effort how many sub del inserts are needed to change one to another
  -latent variables - hidden variables
  
