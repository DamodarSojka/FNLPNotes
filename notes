mostly supervised learning of probabilitistic models
EXEPCTION: EM for spelling correction and POS taggining is unsupervised

Topics in common with previous years
-Corpora, annotation, evaluation
-Ambiguity at all levels
-N-gram models, entropy, smoothing
-Noisy channel framework
-Spelling correction, edit distance
-HMMs, part-of-speech tagging
-Syntax, parsing algorithms, PCFGs, other grammar formalisms
-Lexical semantics:  word senses, semantic roles, distributional semantics

New since last year
-Updated discussion of evaluation, including crowdsourcing
-High-level overview of more modern smoothing methods
-More complete example of spelling correction (end-to-end system)
-Generalized discussion of EM (showing application in both spelling correction and HMMs)
-Text classification (tasks and methods)
-Dependency grammar and related algorithms
-More detail on semantic roles and distributional semantics

Eliminated since last year
-corpus markup
-mathematical details of backoff in N-gram models
-details of forward-backward algorithm for HMMs
-feature structure grammars
-pronoun resolution
-discourse coherence
-Machine Translation (PROBABLY)

Multilevel Ambiguity
  -word sense
  -part of speech
  -syntactic structure I saw a man with a telescope
  -Quantifier scope (what is the scope of the quantifier like some all in the sentence)
  -Multiple: I saw her duck jej kaczke, ja robiaca jak kaczka
  
ZIPFS LAW
  the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word
  Regardless  of  how  large  our  corpus  is,  there  will  be  a  lot  of  infrequent  (and zero-frequency!) word
  
N-gram model (and LMs?)
  probability of sentence, probability of sequence of characters
  used for:
    -spelling correction 
    -automatic speech recognition
    -machine translation
    -predictive text completion
  how to use:
    -for sentence, probability of words of the sentence, word given rest of the words
    -use n-grams the probability of a word only depends on a fixed number of previous words (history)
    
    -P(mast | before, the) = C(before, the, mast)/C(before, the) C- Counts (just an integer)
    
    -to capture beginning/end of sequence augument the input <s> <s> FIRSTWORD when trigram
    -or treat all the things as a long squence P(He | LASTWORDOFPREVIOUSSENTENCE, .)
    
    -use negative log probabilities (called costs), add stuff instead of mult
  
MLE
  problems:
    -thinks that everything that hasn't occured will never occur
    -so if data is sparse (most of the time) don't use it
  
EVALUATION TYPES
  -Extrinsic:
    
  -Intrinsic:
    characterise performance according to the gold standard
    
Entropy and Cross-entropy
  -how many yes/no questions do we need to ask to find out the outcome
  -low entropy means low uncertainty
  -cross-entropy >= entropy, uncertainty of a model can be no less than the true uncertainty
  
  per-word cross-entropy is well approximated by:
    H(w1, w2, ... wn) = -1/n log(base2)Prob(w1, w2, ... wn)
    -1/7(lg2(P(I)) + lg2(P(spent|I) + ....) = -1/7(-0.1 -6.3 ...) = 6
    
  -perplexity:
    -LM performance is reported as perplexity which is 2^cross-entropy
    -The   average   branching   factor   at   each   decision   point,   if   our distribution were uniform ?!?!?!??!?
  
  -low cross-entropy can mean that model is good or the corpus is 'easy', only compare diff models on same corpus
  
Smoothing:
  -MLE makes the training data maximally  probable by making unseen data minimally probable
  -Add-One Laplace:
    -in large vocabulary size it steals to much from seen events
  -Add-alpha (Lidstone):
    -add aplha<1 instead of 1
    -how to pick alpha:
      -three-way split:
        -training data 80-90
        -held-out (or development) 5-10
        -test 5-10
      -train with different aplhas on training set
      -chose alpha that minimizes cross-entropy on dev (held-out) set
      -report final on test set
  -Good-Turing:
    -changes nominator
    -P(GT) = c*/n   c* = (c+1)(N(c+1))/N(c) N(c) ow many things occured with frequency c
      Sam I am I am Sam I do not eat
        I 3          N(1) = 3  3/10 (N(1) / total)   total 1*3 + 2*2 + 1 *3 = 10
        sam 2        N(2) = 2
        am 2         N(3) = 1 
        do 1         so for zero frequency c* = 1(3)/10 (c+1)(N(c+1))/N(c) N(c) ow many things occured with frequency c
        not 1        so for one frequency c* = 2(2)/10
        eat 1        sp fpr two
    -problems with Good-Turing:
      -assumes that we know the vocabulary size
      -doesn't allow "holes" in the counts, N(100) = 0, how to estimate N(99)???
      -applies discounts (lower probability) even to high-frequency items
  -Interpolation:
    -combine higher and lower order N-grams:
      -high-order has better context but sparse
      -low-order limited context but good counts
    -P(INTERPOLATION)(three|I, spent) = lamba1*P(three) + lambda2*P(three|spent) + lambda3*P(three|I, spent)
    -lambda1 + lambda2 + ... + lambdan = 1
    -lambdas are called interpolation parameters or mixture weights
    -chose lamda that maximises perplexity (2^cross-entropy) on held-out (dev)
  -Back-Off:
    -if highest N-gram is present use it, if not pick lower order N-gram
    -each N-gram probability has to be weighted with backoff weight
    -weights have to add up to 1, complicated maths
    problems:
      -bigram New York high probability York is almost always following New so as an N-gram it should have low probability, it is not the case with backoff (and interpolation)
  -Kneser-Ney:
    -takes diversity of histories into account
    -N(1+)(dot_wi) = |{w(1-i) : c(w(1-i), wi) > 0}|
    -take maximum likelihood and replace raw counts with count of histories
  
Word similarity: 
  -two words C(salmon) >> C(swordfish) can P(salmon|caught two) can us anything about P(swordfish|caught two) ?
  -early idea:
    define class of words
  -now:
    -distributed language models
    -each word represented as high-dimensional vector (50-500)
    -similar words represtend by similar vectors
    -learn words representation (embeddings) such that words that behave similarly are close together in space
    -learning embeddings is extremly time-consuming
    -embeddings capture both semantic and syntactic similarity
    
Noisy channel model:
  -assume noise:
    -typed words as opposed to intended words
    -spoken words as opposed to acoustic signal(that is perfect)
    -L1 to L2 (machine translation)
  -argmax(y)P(y|x) = argmax(y)P(x|y)P(y)    P(y) distribution of words user intended to type  P(x) distribution of words user typed
  
Spelling correction system:
  -assume large dict of real words
  -generate list of words y that differ by z character from x
  -Compute P(x|y)P(y) for each y and return max y
  -assume that typed character xi depends only on intended character yi (ignore context) then:
    P(actualword|intendedword) = P(no|not) = P(n|n)P(o|o)P(-|t)
    
  -we want to get rid of hand alignments:
    -we can produce them using MED
    -but to estimate cost we need noise model
    -but until we have alignments we can't esimate the noise model
    -need to use EM (expectation maximization)
    
Expectation-Maximization:
  -initialize learning parameters to arbitrary values (set all costs = 1)
  -compute optimal values for variables (run MED to get alignments)
  -recompute parameters using computed values
  -repeat until convergence (to local optimum of likeligood function)

Likelihood function:
  -call parameters of the model alpha
  -we can calculate probability of the dataset P(data|alpha) (this is the likelihood)
  
Minimum edit distance:
  -amount of deletion + substitution + insertion, MED(stall, table) = 3
    S T A L L -
    d | | s | i
    - T A B L E
  -use dynamic programming to find optimal solution (there are multiple) cost is O(len(str1)* len(str2)) for mem and space
  -used for spelling correction

Definitions:
  -gold labels (when performing Sentiment Analysis) are +/- attached to the sentence
  -estimation vs probability, estimation - we have not true probabilities we throw coin 10 times and mesaure
  -alignmens - misspelled word to real word effert - effort how many sub del inserts are needed to change one to another
