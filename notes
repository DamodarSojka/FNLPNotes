mostly supervised learning of probabilitistic models
EXEPCTION: EM for spelling correction and POS taggining is unsupervised

Topics in common with previous years
-Corpora, annotation, evaluation
-Ambiguity at all levels
-N-gram models, entropy, smoothing
-Noisy channel framework
-Spelling correction, edit distance
-HMMs, part-of-speech tagging
-Syntax, parsing algorithms, PCFGs, other grammar formalisms
-Lexical semantics:  word senses, semantic roles, distributional semantics

New since last year
-Updated discussion of evaluation, including crowdsourcing
-High-level overview of more modern smoothing methods
-More complete example of spelling correction (end-to-end system)
-Generalized discussion of EM (showing application in both spelling correction and HMMs)
-Text classification (tasks and methods)
-Dependency grammar and related algorithms
-More detail on semantic roles and distributional semantics

Eliminated since last year
-corpus markup
-mathematical details of backoff in N-gram models
-details of forward-backward algorithm for HMMs
-feature structure grammars
-pronoun resolution
-discourse coherence
-Machine Translation (PROBABLY)

Multilevel Ambiguity
  -word sense
  -part of speech
  -syntactic structure I saw a man with a telescope
  -Quantifier scope (all in a sentence what it applies to)
  -Multiple: I saw her duck jej kaczke, ja robiaca jak kaczka
  
ZIPFS LAW
  the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word
  Regardless  of  how  large  our  corpus  is,  there  will  be  a  lot  of  infrequent  (and zero-frequency!) word
  
N-gram model (and LMs?)
  probability of sentence, probability of sequence of characters
  used for:
    -spelling correction 
    -automatic speech recognition
    -machine translation
    -predictive text completion
  how to use:
    -for sentence, probability of words of the sentence, word given rest of the words
    -use n-grams the probability of a word only depends on a fixed number of previous words (history)
    
    -P(mast | before, the) = C(before, the, mast)/C(before, the) C- Counts (just an integer)
    
    -to capture beginning/end of sequence augument the input <s> <s> FIRSTWORD when trigram
    -or treat all the things as a long squence P(He | LASTWORDOFPREVIOUSSENTENCE, .)
    
    -use negative log probabilities (called costs), add stuff instead of mult
  
MLE
  problems:
    -thinks that everything that hasn't occured will never occur
    -so if data is sparse (most of the time) don't use it
  
EVALUATION TYPES
  -Extrinsic:
    
  -Intrinsic:
    characterise performance according to the gold standard
    
Entropy and Cross-entropy
  -how many yes/no questions do we need to ask to find out the outcome
  -low entropy means low uncertainty
  -cross-entropy >= entropy, uncertainty of a model can be no less than the true uncertainty
  
  per-word cross-entropy is well approximated by:
    H(w1, w2, ... wn) = -1/n log(base2)Prob(w1, w2, ... wn)
    -1/7(lg2(P(I)) + lg2(P(spent|I) + ....) = -1/7(-0.1 -6.3 ...) = 6
    
  -perplexity:
    -LM performance is reported as perplexity which is 2^cross-entropy
    -The   average   branching   factor   at   each   decision   point,   if   our distribution were uniform ?!?!?!??!?
  
  -low cross-entropy can mean that model is good or the corpus is 'easy', only compare diff models on same corpus
  
Smoothing:
  -MLE makes the training data maximally  probable by making unseen data minimally probable
  -Add-One Laplace:
    -in large vocabulary size it steals to much from seen events
  -Add-alpha (Lidstone):
    -add aplha<1 instead of 1
    -how to pick alpha:
      -three-way split:
        -training data 80-90
        -held-out (or development) 5-10
        -test 5-10
      -train with different aplhas on training set
      -chose alpha that minimizes cross-entropy on dev set
      -report final on test set
  -Good-Turing:
    -changes nominator
    -P(GT) = c*/n   c* = (c+1)(N(c+1))/N(c) N(c) ow many things occured with frequency c
      Sam I am I am Sam I do not eat
        I 3          N(1) = 3
        sam 2        N(2) = 2
        am 2         N(3) = 1
        do 1
        not 1
        eat 1
  
  
Definitions:
  -gold labels with Sentiment Analysis attached +/
  -estimation vs probability, estimation - we have not true probabilities we throw coin 10 times and mesaure
